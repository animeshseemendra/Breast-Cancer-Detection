{
  "cells": [
    {
      "metadata": {
        "_uuid": "869470b74808fecfb2151949645883e5ba392c3f"
      },
      "cell_type": "markdown",
      "source": "**Breast Cancer Detection**\n![](https://blogs.nvidia.com/wp-content/uploads/2018/01/AI_Mammographie.jpg)"
    },
    {
      "metadata": {
        "_uuid": "a3bcdd60937528578137125c74b1b0b2f0d97944"
      },
      "cell_type": "markdown",
      "source": "***Domain Background*** : \n\tBreast Cancer is the most common type of cancer in woman worldwide accounting for 20% of all cases.\n    \n>     In 2012 it resulted in 1.68 million new cases and 522,000 deaths.\n    \nOne of the major problems is that women often neglect the symptoms, which could cause more adverse effects on them thus lowering the survival chances. In developed countries, the survival rate is although high, but it is an area of concern in the developing countries where the 5-year survival rates are poor. In India, there are about one million cases every year and the five-year survival of stage IV breast cancer is about 10%. Therefore it is very important to detect the signs as early as possible. \n    \n>     Invasive ductal carcinoma (IDC) is the most common form of breast cancer.\n   \n   About 80% of all breast cancers are invasive ductal carcinomas. Doctors often do the biopsy or a scan if they detect signs of IDC. The cost of testing for breast cancer sets one back with $5000, which is a very big amount for poor families and also manual identification of presence and extent of breast cancer by a pathologist is critical. Therefore automation of detection of breast cancer using Histopathology images could reduce cost and time as well as improve the accuracy of the test. This is an active research field lot of research papers and articles are present online one that I like is -(https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5453426/) as they used deep learning approach to study on histology images and achieved the sensitivity of 95 which is greater than many pathologists (~90). This shows the power of automation and how it could help in the detection of breast cancer.\n\n"
    },
    {
      "metadata": {
        "_uuid": "363ca7f897d7417a6e745af76993b7f63daa3e87"
      },
      "cell_type": "markdown",
      "source": "***Problem Statement***: \nThe idea is to use pathology test images and classify them as IDC(+) and IDC(-). Accurately identifying and categorizing breast cancer subtypes is an important clinical task, and automated methods can be used to save time and reduce error. The pathological tests include images of the tissues, the task is to train a computer to use these images and respond on whether the person is IDC(+) or IDC(-). Since it is a medical field problem it is important that sensitivity of the output should be high. \n"
    },
    {
      "metadata": {
        "_uuid": "b3cf9c95ee41c98cdae9363d10a9d0d80caa8406"
      },
      "cell_type": "markdown",
      "source": "***Solution Statement***:\n\tOur data involves images with the classes written on data file name, therefore, we would need to extract the class name from it and create a column to store them. We also need to split the dataset into the training set, validation set and testing set. Testing set for checking how good the model works on completely unseen data and validation set to check and avoid underfit or overfit, the will also help to select the best model. One hot encoding will be done in classes column so that it could work better with our model. Image processing step is also required to reduce the pixel range from 0-250 to 0-1. After it CNN model is to be used to predict the class, CNN creates an effective architecture the 2D structure of the image, therefore, it would be the best to use, considering that we are working with the images.\n"
    },
    {
      "metadata": {
        "_uuid": "ab98f3bda4aba704fb1ef93760dbc5135fe244e5"
      },
      "cell_type": "markdown",
      "source": "***Evaluation Metrics***:\n\tThe performance of the model will be evaluated using ROC curve and confusion matrix.  A receiver operating characteristic curve, i.e., ROC curve, is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. The true-positive rate is also known as sensitivity, recall or probability of detection in machine learning. The false-positive rate is also known as the fall-out or probability of false alarm and can be calculated as (1 − specificity). It uses the concept of true positive, true negative, false positive and false negative.\n    \n> * Sensitivity =                True Positive /(True Positive + False Negative)\n> * Recall    =                   True Positive/(True Positive + False Negative)               .                  \n> * Specificity =                True Negative /(  True Negative + False Positive)              .\n> * Precision =                True Positive/ ( True Positive + False Positive)\n                     \nThe perfect classification has the area under the ROC curve equal to 1. Therefore closer the area of our ROC curve to 1 better would be our model. The third is a confusion matrix, it is a two by two table that contains four outcomes produced by a binary classifier. Various measures, such as error-rate, accuracy, specificity, sensitivity, and precision, are derived from the confusion matrix. Sensitivity can be calculated from the confusion matrix, which is important to know when we work in the medical domain i.e how many of the patients were told about having breast cancer our of how many were actually having it.\nThe ROC curve and confusion matrix would be a good evaluation matrix because they both are used for binary classification and our data is also based on binary classification. These metrics could help in evaluating the model through sensitivity, specificity, recall and precision which all are important and are always considered while working in this domain with it would provide us with the visualization of the correctness of the model.\n\n"
    },
    {
      "metadata": {
        "_uuid": "8169af97a70080589e016fc9a218ae3b527d9f2f"
      },
      "cell_type": "markdown",
      "source": "**IMPORT FILES**"
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ee3c9d52a96ca46ccefee28c856f176e7fdc5afa"
      },
      "cell_type": "markdown",
      "source": "\n**Local Directory**"
    },
    {
      "metadata": {
        "_uuid": "4c804a48c43d5eafdd25f3544af52c495b5062dd"
      },
      "cell_type": "markdown",
      "source": "We explore the name of the directory inside which our datafiles are present."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "767ce7de7da5c8fb6dda190fc704f4917a26a0e8"
      },
      "cell_type": "code",
      "source": "print(os.listdir(\"../input\"))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "20071dff7c1d1a2d5380b4a7cdec66e5e48193da"
      },
      "cell_type": "markdown",
      "source": "**Data Exploration**"
    },
    {
      "metadata": {
        "_uuid": "e4b4d6d49239abff9afaa7cf98572e4d7b7002c1"
      },
      "cell_type": "markdown",
      "source": "In data exploration we will first check the name of the files."
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from glob import glob\nData = glob('../input/IDC_regular_ps50_idx5/**/*', recursive=True)    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "303e5e4c3149f552eaf65969d8751162b015a730"
      },
      "cell_type": "code",
      "source": "print(Data[50])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "191b3285e78a5e3e0ea0d90dd99cad1d70071c4d"
      },
      "cell_type": "markdown",
      "source": "Second step  is that we need to check whether all files are images or not"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7a051c1eb2bbf160f3b843090b222eb491604134"
      },
      "cell_type": "code",
      "source": "extention=list()\nfor image in Data:\n    ext=image[-3:]\n    if ext not in extention:\n        extention.append(ext)\nprint((extention))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "87662d907a9f7fdc6d41a4b127064e8d67b70fff"
      },
      "cell_type": "markdown",
      "source": "> **Code Conclusion** : We can see that there are many files along with images but we need to extract only images."
    },
    {
      "metadata": {
        "_uuid": "db88ea74c54bbf50c1d20a55a726c465cefeaf90"
      },
      "cell_type": "markdown",
      "source": "We can see that the extentions are mostly just numbers, therefore we will exclude them and check for extentions that are alphabets"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bec860a9ad5cd869f3146a4edd95562c81b778ad"
      },
      "cell_type": "code",
      "source": "alpha_ext=list()\nfor ex in extention:\n    if ex.isalpha() == True:\n        alpha_ext.append(ex)\nprint(alpha_ext)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1dce201580490b848b95bea97ef9ac0c83519321"
      },
      "cell_type": "markdown",
      "source": "> **Code Conclusion :**  There are only png extentions which are present in alphabets therefore it means that we have only one image extention files with *.png* extentions."
    },
    {
      "metadata": {
        "_uuid": "8ce582bf0b7121f6d42217f81461d3d8bb23c8aa"
      },
      "cell_type": "markdown",
      "source": "Now we need to remove all the other files that we have imported"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a8b8fdf7fb4e8f4fcd2fe3e079fdb15f007f710c"
      },
      "cell_type": "code",
      "source": "Data = glob('../input/IDC_regular_ps50_idx5/**/*.png', recursive=True) ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ab36669bf059f1cf8087e54e29bde7a2b5068c0b"
      },
      "cell_type": "code",
      "source": "print(len(Data))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "15f3df0714e745ec1334e864c6b22656b7f72a74"
      },
      "cell_type": "markdown",
      "source": "> **Code Conclusion **: We have total of 277524 image files"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fdf8eb91f05d6230296955c0159788d5d6bcf676"
      },
      "cell_type": "markdown",
      "source": "Third step is that we need to check where dimentions of all the images are same or not"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "20bcd3ac4577194f799930dd693dbddc5249fbe5"
      },
      "cell_type": "code",
      "source": "\n'''from PIL import Image\nfrom tqdm import tqdm\ndimentions=list()\nfor images in tqdm(Data):\n    dim = Image.open(images)\n    if dim not in dimentions:\n        dimentions.append(dim)\nprint(dimentions)'''\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "471190e7321837531cabbdbd747b30ab7e90d493"
      },
      "cell_type": "markdown",
      "source": "> ***Code Conclusion : *** We can see that the dimentions of images are not equal therefore we would make it all equal ."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3e5cb6052974c45baa373bc359fd9cf3de037dfa"
      },
      "cell_type": "code",
      "source": "import cv2\nimport matplotlib.pyplot as plt\ndef view_images(image):\n    image_cv = cv2.imread(image)\n    plt.imshow(cv2.cvtColor(image_cv, cv2.COLOR_BGR2RGB));\nview_images(Data[52])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a672e046dce3b7eb9d08d50ab84c51e77a608a91"
      },
      "cell_type": "markdown",
      "source": "> ***Code Conclusion :*** We can see that images are very small, though they are cropped images, its hard for human eye to understand them without using some high costly machines. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a893bb680a5d09a515a7667e2674301854740c73"
      },
      "cell_type": "code",
      "source": "def plot_images(photos) :\n    x=0\n    for image in photos:\n        image_cv = cv2.imread(image)\n        plt.subplot(5, 5, x+1)\n        plt.imshow(cv2.cvtColor(image_cv, cv2.COLOR_BGR2RGB));\n        plt.axis('off');\n        x+=1\nplot_images(Data[:25])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f8aabaa38c1a50254707db7fef47b3a3c5a0377f"
      },
      "cell_type": "markdown",
      "source": "Now lets look at the color ranges that our images have"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "839444bac11b227af1fba99858b48153a750e5e6"
      },
      "cell_type": "code",
      "source": "def hist_plot(image):\n    img = cv2.imread(image)\n    plt.subplot(2, 2,1)\n    view_images(image)\n    plt.subplot(2, 2,2)\n    plt.hist(img.ravel()) \nhist_plot(Data[25])\n    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e8c26432e848dcfceadc5fffd30e5957829e75c5"
      },
      "cell_type": "markdown",
      "source": "> ***Code Conclusion :*** From the above image we can conclude that brighter region is more than the darken region in our image.  "
    },
    {
      "metadata": {
        "_uuid": "c051f53cfa6cb1fb2a7d498695bc3b982230fef6"
      },
      "cell_type": "markdown",
      "source": "***Data Extraction***"
    },
    {
      "metadata": {
        "_uuid": "7c6d928a4c2dd217eacbce80679bfb3e21631585"
      },
      "cell_type": "markdown",
      "source": "Next step is we need to extract the class names in which each files belong from its file names. We will save it in output.csv file."
    },
    {
      "metadata": {
        "_uuid": "fd265e222b30dbb9b5d7395a7bcd972c61121a4c",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from tqdm import tqdm\nimport csv\nData_output=list()\nData_output.append([\"Classes\"])\nfor file_name in tqdm(Data):\n    Data_output.append([file_name[-10:-4]])\nwith open(\"output.csv\", \"w\") as f:\n    writer = csv.writer(f)\n    for val in Data_output:\n        writer.writerows([val])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e612b66edd46c1fd3dd4be271803ef516a110bbb"
      },
      "cell_type": "markdown",
      "source": "Below code reads the data from output.csv and displays it"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4aaf1af0ce13cec8530e95783155c0eb9eb53eb5"
      },
      "cell_type": "code",
      "source": "from IPython.display import display # Allows the use of display() for DataFrames\ndata_output = pd.read_csv(\"output.csv\")\ndisplay(data_output.head(5))\nprint(data_output.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "36a676d4144f64b7a8985c15a9433b8117df9236"
      },
      "cell_type": "markdown",
      "source": "> *Class1* represents** IDC(+)** and* Class0* represents** IDC(-)**"
    },
    {
      "metadata": {
        "_uuid": "d35f16c3041dd4923ad7eb84fc9360a2546becd2"
      },
      "cell_type": "markdown",
      "source": "**Data Visualization**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ce678c582dbba33b36a3d4af730d2e10cc3c0596"
      },
      "cell_type": "code",
      "source": "def class_output(images,x):\n    display(data_output.loc[50])\n    view_images(images)\nclass_output(Data[50],50) ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "66336def701f5917baf54ec74c0c250dad6356e4"
      },
      "cell_type": "code",
      "source": "def vis_data(photos,a) :\n    x=0\n    beta=0\n    for image in photos:\n        image_cv = cv2.imread(image)\n        plt.figure(figsize=(50,50))\n        plt.subplot(2, 5, x+1)\n        plt.title('IDC(+)'if data_output.loc[beta]== 'class1' else 'IDC(-)')\n        plt.imshow(cv2.cvtColor(image_cv, cv2.COLOR_BGR2RGB));\n        plt.axis('off');\n        \n        x+=1\n        beta+=1\nplot_images(Data[0:20])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "eab510947c7e73fa8c39072918aa71878d70e88b"
      },
      "cell_type": "code",
      "source": "class1 = data_output[(data_output[\"Classes\"]==\"class1\" )].shape[0]\nclass0 = data_output[(data_output[\"Classes\"]==\"class0\" )].shape[0]\nobjects=[\"class1\",\"class0\"]\ny_pos = np.arange(len(objects))\ncount=[class1,class0]\nplt.bar(y_pos, count, align='center', alpha=0.5)\nplt.xticks(y_pos, objects)\nplt.ylabel('Number of images')\nplt.title('Class distribution')\n \nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c41145257446bde7628e64791c3ca6bf75ca284f"
      },
      "cell_type": "markdown",
      "source": "> ***Code Conclusion :*** We have more number of images in class0 than in class1"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9fa2c30ddd3cd41f368cba48ecc8fb175c9f0e4d"
      },
      "cell_type": "code",
      "source": "percent_class1=class1/len(Data)\npercent_class0=class0/len(Data)\nprint(\"Total Class1 images :\",class1)\nprint(\"Total Class0 images :\",class0)\nprint(\"Percent of class 0 images : \", percent_class0*100)\nprint(\"Percent of class 1 images : \", percent_class1*100)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3b641c8107c7d2a1553c96f03aa478d5344c01c5"
      },
      "cell_type": "markdown",
      "source": "> ***Data Processing  *** "
    },
    {
      "metadata": {
        "_uuid": "ed48663a72034b024f7d2094f96b15fa638c7d33"
      },
      "cell_type": "markdown",
      "source": "We would encode our output data which is present as Class1 and Class0 to 1 and 0 repectively to make it work better with our algorithms."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ce2607a95dcc16bee6fa3297d10d7e4467baa2aa"
      },
      "cell_type": "code",
      "source": "data_output=pd.get_dummies(data_output)\ndisplay(data_output.head(5))\nprint(data_output.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "875532ba6fd63b00d930ff4af396407f1bc9298c"
      },
      "cell_type": "markdown",
      "source": ""
    },
    {
      "metadata": {
        "_uuid": "6ef7e8eb79a1f7c7b8d0ad65b6561de1e873bccf"
      },
      "cell_type": "markdown",
      "source": "Next step is that we need to split our data in train and test . Since our data is uneven we will use statify along with our train_test_split."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5bf34dad9ab39078dee31241060fcdfea907ffb7"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2aa3c9963d7d40308de1f77371f59d6f296f364f",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "data=list()\nfor img in tqdm(Data):\n    image_ar = cv2.imread(img)\n    data.append(cv2.resize(image_ar,(50,50),interpolation=cv2.INTER_CUBIC))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "443c4bd40b9ccf1f05eb3079ddb665eae49f9706"
      },
      "cell_type": "code",
      "source": "\"\"\"%env JOBLIB_TEMP_FOLDER=/tmp\nwith open(\"output_proccess.csv\", \"w\") as f:\n    writer = csv.writer(f)\n    for val in tqdm(data):\n        writer.writerows([val])\"\"\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6845fdb75b51ee19fca710994b4b5602f336ed5b"
      },
      "cell_type": "code",
      "source": "\"\"\"data = pd.read_csv(\"output_proccess.csv\")\"\"\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "46d3aeba9540dbb61afa6636cf52bc79f0212a63"
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\ndata=np.array(data)\nX_train, X_test, Y_train, Y_test = train_test_split(data, data_output, stratify=data_output)\nprint(\"Number of train files\",len(X_train))\nprint(\"Number of test files\",len(X_test))\nprint(\"Number of train_target files\",len(Y_train))\nprint(\"Number of  test_target  files\",len(Y_test))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a141413764d924f87aabd4846dfa3a0ca1f48be7"
      },
      "cell_type": "markdown",
      "source": "We also need a validation set inorder to check overfitting. We can do two things either split test set further into valid set or split train se into valid set."
    },
    {
      "metadata": {
        "_uuid": "28efd4e82e1613ba589c2b5c44abb9d37e0829b0"
      },
      "cell_type": "markdown",
      "source": "We will go for spliting training set into validation set."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5d8045ba6b743225952d6f4f939f710738d3c49a"
      },
      "cell_type": "code",
      "source": "X_train, X_valid, Y_train, Y_valid = train_test_split(X_train, Y_train, stratify=Y_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3d8442531bb4df813876ee3b793d6dac581c6111"
      },
      "cell_type": "code",
      "source": "print(\"Number of train files\",len(X_train))\nprint(\"Number of valid files\",len(X_valid))\nprint(\"Number of train_target files\",len(Y_train))\nprint(\"Number of  valid_target  files\",len(Y_valid))\nprint(\"Number of test files\",len(X_test))\nprint(\"Number of  test_target  files\",len(Y_test))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "eb4ca971641c91c31a891f93626ca50ed1bd9ea9"
      },
      "cell_type": "markdown",
      "source": "> We need to now preprocess our image file. We change pixels range from 0-255 to 0-1."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "37c432cb33cf7995281fa58bad60e8c5b7c6367f"
      },
      "cell_type": "code",
      "source": "for images in X_train:\n    images=images/255.0\nfor images in X_test:\n    images=images/255.0\nfor images in X_valid:\n    images=images/255.0\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "28a70cfa79fbf9e735ce879e09dfd3b0f7160415"
      },
      "cell_type": "code",
      "source": "print(\"Training Data Shape:\", X_train.shape)\nprint(\"Validation Data Shape:\", X_valid.shape)\nprint(\"Testing Data Shape:\", X_test.shape)\nprint(\"Training Label Data Shape:\", Y_train.shape)\nprint(\"Validation Label Data Shape:\", Y_valid.shape)\nprint(\"Testing Label Data Shape:\", Y_test.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1a0fabfa22a0f1f27a75b21c3ded43ef69cb8923"
      },
      "cell_type": "markdown",
      "source": "Now we have our three sets of train, valid and test. We will now create our benchmark model."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "897971555d96ab879fb26ac31f2ceaf859e4c7a3"
      },
      "cell_type": "markdown",
      "source": "> ***BENCHMARK MODEL: *** A simple CNN model"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a3c35390f65f05441288f7699a99bf876366b836"
      },
      "cell_type": "code",
      "source": "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\nfrom keras.layers import Dropout, Flatten, Dense\nfrom keras.models import Sequential\n\nmodel = Sequential()\nmodel.add(Conv2D(filters=32,kernel_size=(2,2),strides=2,padding='same',activation='relu',input_shape=(50,50,3)))\nmodel.add(Flatten())\nmodel.add(Dense(2, activation='softmax'))\nmodel.summary()",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_3 (Conv2D)            (None, 25, 25, 32)        416       \n_________________________________________________________________\nflatten_3 (Flatten)          (None, 20000)             0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 2)                 40002     \n=================================================================\nTotal params: 40,418\nTrainable params: 40,418\nNon-trainable params: 0\n_________________________________________________________________\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3859e685221a58bb2e03d89deaee86ce57601623"
      },
      "cell_type": "code",
      "source": "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])",
      "execution_count": 43,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "eeadffe38c9ddacca2b449345935ae2b34546749"
      },
      "cell_type": "code",
      "source": "\nfrom keras.callbacks import ModelCheckpoint  \ncheckpointer = ModelCheckpoint(filepath='weights.best.cnn.hdf5', \n                               verbose=1, save_best_only=True)\nmodel.fit(X_train, Y_train, \n          validation_data=(X_valid, Y_valid),\n          epochs=10, batch_size=32, callbacks=[checkpointer], verbose=1)",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Train on 156107 samples, validate on 52036 samples\nEpoch 1/10\n156107/156107 [==============================] - 31s 196us/step - loss: 11.5400 - acc: 0.2840 - val_loss: 11.5422 - val_acc: 0.2839\n\nEpoch 00001: val_loss improved from inf to 11.54217, saving model to weights.best.cnn.hdf5\nEpoch 2/10\n156107/156107 [==============================] - 30s 191us/step - loss: 11.5424 - acc: 0.2839 - val_loss: 11.5422 - val_acc: 0.2839\n\nEpoch 00002: val_loss did not improve from 11.54217\nEpoch 3/10\n156107/156107 [==============================] - 30s 191us/step - loss: 11.5424 - acc: 0.2839 - val_loss: 11.5422 - val_acc: 0.2839\n\nEpoch 00003: val_loss did not improve from 11.54217\nEpoch 4/10\n156107/156107 [==============================] - 29s 189us/step - loss: 11.5424 - acc: 0.2839 - val_loss: 11.5422 - val_acc: 0.2839\n\nEpoch 00004: val_loss did not improve from 11.54217\nEpoch 5/10\n156107/156107 [==============================] - 29s 189us/step - loss: 11.5424 - acc: 0.2839 - val_loss: 11.5422 - val_acc: 0.2839\n\nEpoch 00005: val_loss did not improve from 11.54217\nEpoch 6/10\n156107/156107 [==============================] - 29s 189us/step - loss: 11.5424 - acc: 0.2839 - val_loss: 11.5422 - val_acc: 0.2839\n\nEpoch 00006: val_loss did not improve from 11.54217\nEpoch 7/10\n156107/156107 [==============================] - 30s 189us/step - loss: 11.5424 - acc: 0.2839 - val_loss: 11.5422 - val_acc: 0.2839\n\nEpoch 00007: val_loss did not improve from 11.54217\nEpoch 8/10\n156107/156107 [==============================] - 30s 189us/step - loss: 11.5424 - acc: 0.2839 - val_loss: 11.5422 - val_acc: 0.2839\n\nEpoch 00008: val_loss did not improve from 11.54217\nEpoch 9/10\n156107/156107 [==============================] - 29s 188us/step - loss: 11.5424 - acc: 0.2839 - val_loss: 11.5422 - val_acc: 0.2839\n\nEpoch 00009: val_loss did not improve from 11.54217\nEpoch 10/10\n156107/156107 [==============================] - 30s 189us/step - loss: 11.5424 - acc: 0.2839 - val_loss: 11.5422 - val_acc: 0.2839\n\nEpoch 00010: val_loss did not improve from 11.54217\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 44,
          "data": {
            "text/plain": "<keras.callbacks.History at 0x7fa73a730390>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ee4f6b24af3b8cc3f30f688f5946ab2e5e4f79d1"
      },
      "cell_type": "code",
      "source": "model.load_weights('weights.best.cnn.hdf5')",
      "execution_count": 45,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d73bea28cfe39ae171efbb1f1962794e3449b583"
      },
      "cell_type": "code",
      "source": "predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in tqdm(X_test)]",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": "100%|██████████| 69381/69381 [01:32<00:00, 747.04it/s]\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "99b58df71db1b9d82b8ef074f782d0b5a9004c20"
      },
      "cell_type": "code",
      "source": "test_Y=list()\nclass0=Y_test['Classes_class0'].values.tolist()\nclass1=Y_test['Classes_class1'].values.tolist()\nfor a,b in zip(class0,class1):\n    test_Y.append((a,b))\nprint(np.argmax(test_Y, axis=1))",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[1 0 0 ... 0 1 0]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "92041cade53648ca48c5fc46a9583aba00279fd9"
      },
      "cell_type": "code",
      "source": "test_accuracy = 100*np.sum(np.array(predictions)==np.argmax(test_Y, axis=1))/len(predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Test accuracy: 28.3882%\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bcf108aabff562b7ec9c98e03215615d1f1de2fd"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "abe58283baa85b5507791165f8f1bec120ac90d6"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3d43ed66a2e1e82d7de0ecdd71430fc18551a72e"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dbd4068d79d6f803a7b74a60644d24aa887d935a"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "86dfee2e89a8eddfcaec06824c67f91050afb167"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e82e122eebc1c3aebf2e251ecf2b07a2f0ed965b"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1fe64507d712f745fb3349d5ebd1e1cbda35fc9d"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3ddbfb6298866d0b67cd9807fbdd09bcec297cdf"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a26189c218b868d486f1dcb2a69feec8a147cec6"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2d0645cdc6e1977e91044f237be4f1f56caea19c"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3460eb5d25cccfb3193eb7774fc58add16052bf4"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5cad14630f373ef03db19db0944b886ee01f18b0"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}